import streamlit as st
import pandas as pd
import os
import joblib
import pickle
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import matplotlib.font_manager as fm
from konlpy.tag import Okt
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import platform
import json
import re
from lib.steam_api import get_game_name

st.set_page_config(page_title="ê²°ê³¼ ì‹œê°í™”", page_icon="ğŸ“Š")

def prepare_wordcloud_text(reviews_series):
    okt, processed_text_list = Okt(), []
    for review in reviews_series.dropna():
        tokens = okt.pos(str(review))
        words = [word for word, pos in tokens if (pos == 'Noun' or pos == 'Adjective') and len(word) > 1]
        processed_text_list.append(" ".join(set(words)))
    return " ".join(processed_text_list)

@st.cache_resource
def get_font_path():
    system = platform.system()
    if system == "Windows": font_path = "c:/Windows/Fonts/malgun.ttf"
    elif system == "Darwin": font_path = "/System/Library/Fonts/Supplemental/AppleGothic.ttf"
    else:
        font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
        if not os.path.exists(font_path):
            st.warning("ë‚˜ëˆ”ê³ ë”• í°íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `sudo apt-get install fonts-nanum*` ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
            return None
    if not os.path.exists(font_path):
        st.warning(f"ì§€ì •ëœ í°íŠ¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}"); return None
    return font_path

@st.cache_resource
def load_sklearn_model_and_vectorizer(model_path):
    model = joblib.load(model_path)
    vectorizer_path = os.path.join(os.path.dirname(model_path), os.path.basename(model_path).replace('model_', 'vectorizer_'))
    vectorizer = joblib.load(vectorizer_path)
    return model, vectorizer

@st.cache_resource
def load_lstm_model_and_dependencies(model_dir_path):
    model = load_model(os.path.join(model_dir_path, 'best_model.keras'))
    with open(os.path.join(model_dir_path, 'tokenizer.pkl'), 'rb') as f: tokenizer = pickle.load(f)
    with open(os.path.join(model_dir_path, 'params.json'), 'r', encoding='utf-8') as f: params = json.load(f)
    return model, tokenizer, params

def predict_emotions(model_type, model_objects, data_df):
    if 'document' not in data_df.columns: st.error("'document' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return None
    data_df.dropna(subset=['document'], inplace=True)
    reviews = data_df['document'].astype(str)
    if model_type == "Scikit-learn":
        model, vectorizer = model_objects
        preds = model.predict(vectorizer.transform(reviews))
    else:
        model, tokenizer, params = model_objects
        okt = Okt()
        tokenized_reviews = [okt.morphs(r, stem=True) for r in reviews]
        seqs = tokenizer.texts_to_sequences(tokenized_reviews)
        padded_seqs = pad_sequences(seqs, maxlen=params['max_len'], padding='post')
        predictions = model.predict(padded_seqs)
        pred_indices = np.argmax(predictions, axis=1)
        preds = [params['final_text_labels'][i] for i in pred_indices]
    data_df['predicted_label'] = preds
    return data_df

def load_stopwords_from_file(filepath):
    if not os.path.exists(filepath) or os.path.getsize(filepath) == 0: return set()
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            if filepath.endswith('.json'): return set(json.load(f))
            elif filepath.endswith('.txt'): return {line.strip() for line in f if line.strip()}
    except json.JSONDecodeError:
        st.error(f"'{os.path.basename(filepath)}' íŒŒì¼ì˜ JSON í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. `[\"ë‹¨ì–´1\", \"ë‹¨ì–´2\"]` í˜•ì‹ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return set()
    except Exception as e:
        st.error(f"'{os.path.basename(filepath)}' íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return set()
    return set()

# <<-- ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘: save_stopwords_to_file í•¨ìˆ˜ ê°•í™” -->>
def save_stopwords_to_file(filepath, stopwords_set):
    """ë¶ˆìš©ì–´ ì„¸íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ê²½ë¡œê°€ ì—†ì–´ë„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        dir_path = os.path.dirname(filepath)
        # ë””ë ‰í† ë¦¬ ê²½ë¡œê°€ ì¡´ì¬í•  ê²½ìš°ì—ë§Œ í´ë” ìƒì„± ì‹œë„
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(sorted(list(stopwords_set)), f, ensure_ascii=False, indent=4)
        return True
    except Exception as e:
        st.error(f"'{filepath}' íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
# <<-- ìˆ˜ì •ëœ ë¶€ë¶„ ë -->>

font_path = get_font_path()
if font_path:
    font_prop = fm.FontProperties(fname=font_path)
    plt.rcParams['font.family'] = font_prop.get_name()
    plt.rcParams['axes.unicode_minus'] = False

st.title("ğŸ“Š ê°ì„± ë¶„ì„ ê²°ê³¼ ì‹œê°í™”")
st.markdown("í•™ìŠµëœ ëª¨ë¸ê³¼ ë¦¬ë·° ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.")

models_dir, data_dir = 'models', 'data'
if not os.path.exists(models_dir): os.makedirs(models_dir)
if not os.path.exists(data_dir): os.makedirs(data_dir)
sklearn_model_files = sorted([f for f in os.listdir(models_dir) if f.endswith('.pkl')])
lstm_model_dirs = sorted([d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d)) and 'best_model.keras' in os.listdir(os.path.join(models_dir, d))])
data_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.csv')])

st.subheader("1. ë°ì´í„° ì„ íƒ")
if not data_files: st.warning("ë¶„ì„í•  ë°ì´í„° íŒŒì¼ì´ 'data' í´ë”ì— ì—†ìŠµë‹ˆë‹¤."); st.stop()
selected_data = st.selectbox("ë¶„ì„í•  ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”:", data_files)
data_path = os.path.join(data_dir, selected_data)
app_id_match = re.search(r'(\d+)', selected_data)
app_id = app_id_match.group(1) if app_id_match else None
if app_id: st.info(f"ë¶„ì„ ëŒ€ìƒ ê²Œì„: **{get_game_name(app_id)}** (App ID: {app_id})")

st.subheader("2. ëª¨ë¸ ì„ íƒ")
model_type = st.radio("ì‹œê°í™”ì— ì‚¬ìš©í•  ëª¨ë¸ ì¢…ë¥˜:", ("Scikit-learn", "Deep Learning (LSTM)"), horizontal=True)
model_to_load = None
if model_type == "Scikit-learn":
    if not sklearn_model_files: st.warning("í•™ìŠµëœ Scikit-learn ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    else: selected_model_file = st.selectbox("ëª¨ë¸(.pkl)ì„ ì„ íƒí•˜ì„¸ìš”:", sklearn_model_files); model_to_load = os.path.join(models_dir, selected_model_file)
else:
    if not lstm_model_dirs: st.warning("í•™ìŠµëœ LSTM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    else: selected_model_dir = st.selectbox("ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:", lstm_model_dirs); model_to_load = os.path.join(models_dir, selected_model_dir)

st.subheader("3. ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì •")
with st.expander("ë¶ˆìš©ì–´ ëª©ë¡ ìˆ˜ì • ë° ê´€ë¦¬"):
    default_stopwords_set = load_stopwords_from_file("default_stop_words.json")
    st.markdown(f"**ê¸°ë³¸ ë¶ˆìš©ì–´ ({len(default_stopwords_set)}ê°œ)**: `default_stop_words.json`")
    st.write(sorted(list(default_stopwords_set)))
    game_stopwords_path = os.path.join("stopwords", f"{app_id}.json") if app_id else None
    game_stopwords_set = load_stopwords_from_file(game_stopwords_path) if game_stopwords_path else set()
    st.markdown(f"**'{get_game_name(app_id)}' ê²Œì„ë³„ ë¶ˆìš©ì–´ ({len(game_stopwords_set)}ê°œ)**")
    game_stopwords_input = st.text_area("ì´ ê²Œì„ì—ë§Œ ì ìš©í•  ë¶ˆìš©ì–´ ìˆ˜ì •:", value=", ".join(sorted(list(game_stopwords_set))), key=f"game_stopwords_{app_id}")
    if st.button("ê²Œì„ë³„ ë¶ˆìš©ì–´ ì €ì¥"):
        if game_stopwords_path:
            new_game_stopwords = {word.strip() for word in re.split(r'[,\n]', game_stopwords_input) if word.strip()}
            save_stopwords_to_file(game_stopwords_path, new_game_stopwords)
            st.success(f"'{game_stopwords_path}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!"); st.rerun()
        else: st.error("App IDë¥¼ íŒŒì¼ëª…ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì–´ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    custom_stopwords_input = st.text_area("ì´ë²ˆ ë¶„ì„ì—ë§Œ ì¼íšŒì„±ìœ¼ë¡œ ì‚¬ìš©í•  ë¶ˆìš©ì–´ ì…ë ¥:")

if model_to_load and selected_data:
    if st.button("ë¶„ì„ ë° ì‹œê°í™” ì‹œì‘", type="primary"):
        game_stopwords_final = {word.strip() for word in re.split(r'[,\n]', game_stopwords_input) if word.strip()}
        temp_stopwords_final = {word.strip() for word in re.split(r'[,\n]', custom_stopwords_input) if word.strip()}
        final_stopwords = default_stopwords_set.union(game_stopwords_final).union(temp_stopwords_final)
        st.write(f"**ì´ {len(final_stopwords)}ê°œì˜ ë¶ˆìš©ì–´ ì ìš©:** (ê¸°ë³¸ + ê²Œì„ë³„ + ì„ì‹œ)")
        try:
            with st.spinner("ëª¨ë¸ ë¡œë“œ ì¤‘..."):
                if model_type == "Scikit-learn": model_objects = load_sklearn_model_and_vectorizer(model_to_load)
                else: model_objects = load_lstm_model_and_dependencies(model_to_load)
            with st.spinner("ë°ì´í„° ì˜ˆì¸¡ ì¤‘..."):
                df = pd.read_csv(data_path)
                result_df = predict_emotions(model_type, model_objects, df)
            if result_df is not None:
                st.subheader("ê°ì„± ë¶„í¬")
                label_counts = result_df['predicted_label'].value_counts()
                fig1, ax1 = plt.subplots()
                colors = ['#4CAF50' if x=='ê¸ì •' else '#F44336' if x=='ë¶€ì •' else '#FFC107' for x in label_counts.index]
                ax1.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=90, colors=colors)
                ax1.axis('equal'); st.pyplot(fig1)
                st.subheader("ì›Œë“œ í´ë¼ìš°ë“œ")
                pos_reviews_text = prepare_wordcloud_text(result_df[result_df['predicted_label'] == 'ê¸ì •']['document'])
                neu_reviews_text = prepare_wordcloud_text(result_df[result_df['predicted_label'] == 'ì¤‘ë¦½']['document'])
                neg_reviews_text = prepare_wordcloud_text(result_df[result_df['predicted_label'] == 'ë¶€ì •']['document'])
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.markdown("<h4 style='text-align: center; color: green;'>ê¸ì • ë¦¬ë·°</h4>", unsafe_allow_html=True)
                    if pos_reviews_text.strip():
                        wc_pos = WordCloud(width=400, height=300, background_color='white', font_path=font_path, stopwords=final_stopwords).generate(pos_reviews_text)
                        fig_pos, ax_pos = plt.subplots(); ax_pos.imshow(wc_pos, interpolation='bilinear'); ax_pos.axis('off'); st.pyplot(fig_pos)
                    else: st.info("ê¸ì • ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                with col2:
                    st.markdown("<h4 style='text-align: center; color: gray;'>ì¤‘ë¦½ ë¦¬ë·°</h4>", unsafe_allow_html=True)
                    if neu_reviews_text.strip():
                        wc_neu = WordCloud(width=400, height=300, background_color='white', colormap='Greys', font_path=font_path, stopwords=final_stopwords).generate(neu_reviews_text)
                        fig_neu, ax_neu = plt.subplots(); ax_neu.imshow(wc_neu, interpolation='bilinear'); ax_neu.axis('off'); st.pyplot(fig_neu)
                    else: st.info("ì¤‘ë¦½ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                with col3:
                    st.markdown("<h4 style='text-align: center; color: red;'>ë¶€ì • ë¦¬ë·°</h4>", unsafe_allow_html=True)
                    if neg_reviews_text.strip():
                        wc_neg = WordCloud(width=400, height=300, background_color='black', colormap='Reds', font_path=font_path, stopwords=final_stopwords).generate(neg_reviews_text)
                        fig_neg, ax_neg = plt.subplots(); ax_neg.imshow(wc_neg, interpolation='bilinear'); ax_neg.axis('off'); st.pyplot(fig_neg)
                    else: st.info("ë¶€ì • ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e: st.error(f"ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")